{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> <span style=\"color:white\">Electricity Sector Data Streaming & Analysis</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> <span style=\"color:white\">GROUP 04</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name                   | SID       | Unikey   |\n",
    "| ---------------------- | --------- | -------- |\n",
    "| Putu Eka Udiyani Putri | 550067302 | pput0940 |\n",
    "| Rengga Firmandika      | 550126632 | rfir0117 |\n",
    "| Vincentius Ansel Suppa | 550206406 | vsup0468 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">0. Configuration and Import Required Libraries</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick start:**\n",
    "1. Project structure:\n",
    "   \n",
    "   <pre>\n",
    "   Assignment2_Tut07_G04/\n",
    "   ├── Assignment_2.ipynb      # main notebook\n",
    "   └── requirements.txt        # list of required libraries to run the notebook\n",
    "   </pre>\n",
    "\n",
    "   Ensure your working directory is writable.\n",
    "\n",
    "2. Create venv & install exact dependencies<br/>\n",
    "   `python -m venv .venv`<br/>\n",
    "   Windows: `.\\.venv\\Scripts\\activate` | macOS/Linux: `source .venv/bin/activate`<br/>\n",
    "   `python -m pip install --upgrade pip`<br/>\n",
    "   `pip install -r requirements.txt`\n",
    "\n",
    "3. Copy `.env.template` to `.env` file, replace `your_api_key` with your actual API key. \n",
    "\n",
    "4. Run the full pipeline (extract -> clean -> augment -> transform -> load)<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the required libraries first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import paho.mqtt.client as mqtt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">1. Data Retrieval</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic configs\n",
    "API_KEY = os.getenv(\"OPENELECTRICITY_API_KEY\")\n",
    "API_KEY = API_KEY.strip().strip('\"').strip(\"'\")  \n",
    "BASE_URL = \"https://api.openelectricity.org.au/v4/\"\n",
    "HEADERS = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "\n",
    "# function to fetch data\n",
    "def fetch_data_from_API(endpoint: str, query_params: dict): \n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}{endpoint}\", headers=HEADERS, params=query_params)\n",
    "        \n",
    "        print(f\"Response status: {response.status_code}\")\n",
    "        print(f\"Response url: {response.url}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"API Error {response.status_code}: {response.text}\")\n",
    "            print(f\"Response headers: {dict(response.headers)}\")\n",
    "\n",
    "            try:\n",
    "                error_json = response.json()\n",
    "                print(f\"Error details: {error_json}\")\n",
    "            except:\n",
    "                print(\"Could not parse error response as JSON\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# helper function to save dataset\n",
    "def save_dataset(df: pd.DataFrame, out_csv_path: str):\n",
    "\tout_path = Path(out_csv_path)\n",
    "\tout_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\tdf.to_csv(out_path, index=False)\n",
    "\tprint(f\"Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">1.1 Get All Facilities in NEM Region</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response status: 200\n",
      "Response url: https://api.openelectricity.org.au/v4/facilities/?network_id=NEM\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "      <th>network_id</th>\n",
       "      <th>network_region</th>\n",
       "      <th>description</th>\n",
       "      <th>units</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>location.lat</th>\n",
       "      <th>location.lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADP</td>\n",
       "      <td>Adelaide Desalination</td>\n",
       "      <td>NEM</td>\n",
       "      <td>SA1</td>\n",
       "      <td>&lt;p&gt;The Adelaide Desalination plant (ADP), form...</td>\n",
       "      <td>[{'code': 'ADPPV1', 'fueltech_id': 'solar_util...</td>\n",
       "      <td>2025-08-05T06:08:12Z</td>\n",
       "      <td>2023-10-18T04:34:30Z</td>\n",
       "      <td>-35.096948</td>\n",
       "      <td>138.484061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDGASF</td>\n",
       "      <td>Aldoga</td>\n",
       "      <td>NEM</td>\n",
       "      <td>QLD1</td>\n",
       "      <td>&lt;p&gt;The Aldoga Solar Farm will be approximately...</td>\n",
       "      <td>[{'code': 'ALDGASF1', 'fueltech_id': 'solar_ut...</td>\n",
       "      <td>2025-03-25T00:52:44Z</td>\n",
       "      <td>2025-01-31T04:19:33Z</td>\n",
       "      <td>-23.839544</td>\n",
       "      <td>151.084900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMCORGR</td>\n",
       "      <td>Amcor Glass</td>\n",
       "      <td>NEM</td>\n",
       "      <td>SA1</td>\n",
       "      <td>&lt;p&gt;&lt;/p&gt;</td>\n",
       "      <td>[{'code': 'AMCORGR', 'fueltech_id': 'distillat...</td>\n",
       "      <td>2023-10-18T04:34:32Z</td>\n",
       "      <td>2023-10-18T04:34:32Z</td>\n",
       "      <td>-34.882663</td>\n",
       "      <td>138.577975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANGASTON</td>\n",
       "      <td>Angaston</td>\n",
       "      <td>NEM</td>\n",
       "      <td>SA1</td>\n",
       "      <td>&lt;p&gt;Angaston Power Station is a diesel-powered ...</td>\n",
       "      <td>[{'code': 'ANGAS1', 'fueltech_id': 'distillate...</td>\n",
       "      <td>2025-09-07T01:53:13Z</td>\n",
       "      <td>2023-10-18T04:34:32Z</td>\n",
       "      <td>-34.503948</td>\n",
       "      <td>139.024296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APS</td>\n",
       "      <td>Anglesea</td>\n",
       "      <td>NEM</td>\n",
       "      <td>VIC1</td>\n",
       "      <td>&lt;p&gt;The Anglesea Power Station was a brown coal...</td>\n",
       "      <td>[{'code': 'APS', 'fueltech_id': 'coal_brown', ...</td>\n",
       "      <td>2024-11-04T00:41:34Z</td>\n",
       "      <td>2023-10-18T04:34:32Z</td>\n",
       "      <td>-38.389031</td>\n",
       "      <td>144.180589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code                   name network_id network_region  \\\n",
       "0       ADP  Adelaide Desalination        NEM            SA1   \n",
       "1   ALDGASF                 Aldoga        NEM           QLD1   \n",
       "2   AMCORGR            Amcor Glass        NEM            SA1   \n",
       "3  ANGASTON               Angaston        NEM            SA1   \n",
       "4       APS               Anglesea        NEM           VIC1   \n",
       "\n",
       "                                         description  \\\n",
       "0  <p>The Adelaide Desalination plant (ADP), form...   \n",
       "1  <p>The Aldoga Solar Farm will be approximately...   \n",
       "2                                            <p></p>   \n",
       "3  <p>Angaston Power Station is a diesel-powered ...   \n",
       "4  <p>The Anglesea Power Station was a brown coal...   \n",
       "\n",
       "                                               units            updated_at  \\\n",
       "0  [{'code': 'ADPPV1', 'fueltech_id': 'solar_util...  2025-08-05T06:08:12Z   \n",
       "1  [{'code': 'ALDGASF1', 'fueltech_id': 'solar_ut...  2025-03-25T00:52:44Z   \n",
       "2  [{'code': 'AMCORGR', 'fueltech_id': 'distillat...  2023-10-18T04:34:32Z   \n",
       "3  [{'code': 'ANGAS1', 'fueltech_id': 'distillate...  2025-09-07T01:53:13Z   \n",
       "4  [{'code': 'APS', 'fueltech_id': 'coal_brown', ...  2024-11-04T00:41:34Z   \n",
       "\n",
       "             created_at  location.lat  location.lng  \n",
       "0  2023-10-18T04:34:30Z    -35.096948    138.484061  \n",
       "1  2025-01-31T04:19:33Z    -23.839544    151.084900  \n",
       "2  2023-10-18T04:34:32Z    -34.882663    138.577975  \n",
       "3  2023-10-18T04:34:32Z    -34.503948    139.024296  \n",
       "4  2023-10-18T04:34:32Z    -38.389031    144.180589  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set endpoint and params\n",
    "ENDPOINT = \"facilities/\"\n",
    "PARAMS = {\n",
    "    'network_id': 'NEM',\n",
    "}\n",
    "\n",
    "# fetch facilities data\n",
    "facilities = fetch_data_from_API(endpoint=ENDPOINT, query_params=PARAMS)\n",
    "facilities_df = pd.json_normalize(facilities['data'])\n",
    "facilities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> <span style=\"color:pink\">1.2 Get All Power Generated and CO2 Emissions per Facility</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facility data will return total data instead of facility specific data if we do not specify the facility code in tha API call. Hence, we need to pass the facility code we get from previous API call to this endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total facilities: 514\n"
     ]
    }
   ],
   "source": [
    "# get all facility code\n",
    "FACILITY_LIST = facilities_df[\"code\"].tolist()\n",
    "print(f\"Total facilities: {len(FACILITY_LIST)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, since the parameter only accept 30 max characters and we have more than 500 facilities, passing all facility code at once will result in error. To get around that, we will use the batching strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./DATA/EXTRACTED\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "ENDPOINT = \"data/facilities/NEM\"\n",
    "batch_size = 5\n",
    "\n",
    "# function chunk the facility code list into batches\n",
    "def chunk_list(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n], i, min(i+n, len(lst))  \n",
    "\n",
    "# batch retrieval\n",
    "def batch_retrieval(batch_size:int):\n",
    "    for batch, start, end in chunk_list(FACILITY_LIST, batch_size):\n",
    "        batch_id = math.ceil(end/batch_size)\n",
    "        cache_path = os.path.join(OUT_DIR, f\"batch_{start+1:04d}_{end:04d}.json\")\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            print(f\"Batch {batch_id}: {start+1}-{end} already cached.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Fetching batch {batch_id}: facilities {start+1}–{end} ({batch})\")\n",
    "\n",
    "        # params\n",
    "        params = {\n",
    "            'network_code': 'NEM',\n",
    "            'metrics': {'power', 'emissions'},\n",
    "            'interval': '5m',\n",
    "            \"date_start\": \"2025-10-01\",\n",
    "            \"date_end\": \"2025-10-08\",\n",
    "            \"facility_code\": {f for f in batch},\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            r = requests.get(f\"{BASE_URL}{ENDPOINT}\", headers=HEADERS, params=params, timeout=90)\n",
    "            if r.status_code == 200:\n",
    "                payload = r.json()\n",
    "                with open(cache_path, \"w\") as f:\n",
    "                    json.dump(payload, f, indent=2)\n",
    "                print(f\"Saved {cache_path}\")\n",
    "            else:\n",
    "                print(f\"HTTP {r.status_code}: {r.text[:150]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Batch {batch_id}: {e}\")\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "# function to flatten the payload\n",
    "def flatten(payload):\n",
    "    rows = []\n",
    "    for block in payload.get(\"data\", []):\n",
    "        metric   = block.get(\"metric\")\n",
    "        unit     = block.get(\"unit\")\n",
    "        interval = block.get(\"interval\")\n",
    "        for res in block.get(\"results\", []):\n",
    "            unit_code = (res.get(\"columns\") or {}).get(\"unit_code\")\n",
    "            for ts, val in res.get(\"data\", []):\n",
    "                rows.append({\"timestamp\": ts, \"unit_code\": unit_code,\n",
    "                            \"metric\": metric, \"interval\": interval, \"unit\": unit, \"value\": val})\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: 1-5 already cached.\n",
      "Batch 2: 6-10 already cached.\n",
      "Batch 3: 11-15 already cached.\n",
      "Batch 4: 16-20 already cached.\n",
      "Batch 5: 21-25 already cached.\n",
      "Batch 6: 26-30 already cached.\n",
      "Batch 7: 31-35 already cached.\n",
      "Batch 8: 36-40 already cached.\n",
      "Batch 9: 41-45 already cached.\n",
      "Batch 10: 46-50 already cached.\n",
      "Batch 11: 51-55 already cached.\n",
      "Batch 12: 56-60 already cached.\n",
      "Batch 13: 61-65 already cached.\n",
      "Batch 14: 66-70 already cached.\n",
      "Batch 15: 71-75 already cached.\n",
      "Batch 16: 76-80 already cached.\n",
      "Batch 17: 81-85 already cached.\n",
      "Batch 18: 86-90 already cached.\n",
      "Batch 19: 91-95 already cached.\n",
      "Batch 20: 96-100 already cached.\n",
      "Batch 21: 101-105 already cached.\n",
      "Batch 22: 106-110 already cached.\n",
      "Batch 23: 111-115 already cached.\n",
      "Batch 24: 116-120 already cached.\n",
      "Batch 25: 121-125 already cached.\n",
      "Batch 26: 126-130 already cached.\n",
      "Batch 27: 131-135 already cached.\n",
      "Batch 28: 136-140 already cached.\n",
      "Batch 29: 141-145 already cached.\n",
      "Batch 30: 146-150 already cached.\n",
      "Batch 31: 151-155 already cached.\n",
      "Batch 32: 156-160 already cached.\n",
      "Batch 33: 161-165 already cached.\n",
      "Batch 34: 166-170 already cached.\n",
      "Batch 35: 171-175 already cached.\n",
      "Batch 36: 176-180 already cached.\n",
      "Batch 37: 181-185 already cached.\n",
      "Batch 38: 186-190 already cached.\n",
      "Batch 39: 191-195 already cached.\n",
      "Batch 40: 196-200 already cached.\n",
      "Batch 41: 201-205 already cached.\n",
      "Batch 42: 206-210 already cached.\n",
      "Batch 43: 211-215 already cached.\n",
      "Batch 44: 216-220 already cached.\n",
      "Batch 45: 221-225 already cached.\n",
      "Batch 46: 226-230 already cached.\n",
      "Batch 47: 231-235 already cached.\n",
      "Batch 48: 236-240 already cached.\n",
      "Batch 49: 241-245 already cached.\n",
      "Batch 50: 246-250 already cached.\n",
      "Batch 51: 251-255 already cached.\n",
      "Batch 52: 256-260 already cached.\n",
      "Batch 53: 261-265 already cached.\n",
      "Batch 54: 266-270 already cached.\n",
      "Batch 55: 271-275 already cached.\n",
      "Batch 56: 276-280 already cached.\n",
      "Batch 57: 281-285 already cached.\n",
      "Batch 58: 286-290 already cached.\n",
      "Batch 59: 291-295 already cached.\n",
      "Batch 60: 296-300 already cached.\n",
      "Batch 61: 301-305 already cached.\n",
      "Batch 62: 306-310 already cached.\n",
      "Batch 63: 311-315 already cached.\n",
      "Batch 64: 316-320 already cached.\n",
      "Batch 65: 321-325 already cached.\n",
      "Batch 66: 326-330 already cached.\n",
      "Batch 67: 331-335 already cached.\n",
      "Batch 68: 336-340 already cached.\n",
      "Batch 69: 341-345 already cached.\n",
      "Batch 70: 346-350 already cached.\n",
      "Batch 71: 351-355 already cached.\n",
      "Batch 72: 356-360 already cached.\n",
      "Batch 73: 361-365 already cached.\n",
      "Batch 74: 366-370 already cached.\n",
      "Batch 75: 371-375 already cached.\n",
      "Batch 76: 376-380 already cached.\n",
      "Fetching batch 77: facilities 381–385 (['SKSF', 'STHBKTEC', 'SVALE', 'STGEORGE', 'SLDCBLK'])\n",
      "HTTP 416: {\"version\":\"4.3.0\",\"response_status\":\"ERROR\",\"error\":\"No data available for facility ['SVALE', 'SLDCBLK', 'SKSF', 'STHBKTEC', 'STGEORGE'] in the speci\n",
      "Batch 78: 386-390 already cached.\n",
      "Batch 79: 391-395 already cached.\n",
      "Batch 80: 396-400 already cached.\n",
      "Batch 81: 401-405 already cached.\n",
      "Batch 82: 406-410 already cached.\n",
      "Batch 83: 411-415 already cached.\n",
      "Batch 84: 416-420 already cached.\n",
      "Batch 85: 421-425 already cached.\n",
      "Fetching batch 86: facilities 426–430 (['THEDROP', '0TILBSF', 'TIMWEST', '0TOMAGOBESS', 'TOORAWF'])\n",
      "HTTP 416: {\"version\":\"4.3.0\",\"response_status\":\"ERROR\",\"error\":\"No data available for facility ['THEDROP', '0TOMAGOBESS', '0TILBSF', 'TOORAWF', 'TIMWEST'] in th\n",
      "Batch 87: 431-435 already cached.\n",
      "Batch 88: 436-440 already cached.\n",
      "Batch 89: 441-445 already cached.\n",
      "Batch 90: 446-450 already cached.\n",
      "Batch 91: 451-455 already cached.\n",
      "Batch 92: 456-460 already cached.\n",
      "Batch 93: 461-465 already cached.\n",
      "Batch 94: 466-470 already cached.\n",
      "Batch 95: 471-475 already cached.\n",
      "Batch 96: 476-480 already cached.\n",
      "Batch 97: 481-485 already cached.\n",
      "Batch 98: 486-490 already cached.\n",
      "Batch 99: 491-495 already cached.\n",
      "Batch 100: 496-500 already cached.\n",
      "Batch 101: 501-505 already cached.\n",
      "Batch 102: 506-510 already cached.\n",
      "Batch 103: 511-514 already cached.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all facility data in batch of 5 per API call\n",
    "batch_retrieval(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">2. Data Integration and Caching</span></b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some facilities have more than one units, so we need to make separate tables for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate the facilities and units rows\n",
    "def build_tables(facilities: list[dict]):\n",
    "    facility_rows: list[dict] = []\n",
    "    unit_rows: list[dict] = []\n",
    "\n",
    "    for f in facilities:\n",
    "        f_code = f.get(\"code\")\n",
    "        facility_rows.append({\n",
    "            \"facility_code\": f_code,\n",
    "            \"facility_name\": f.get(\"name\"),\n",
    "            \"network_id\": f.get(\"network_id\"),\n",
    "            \"network_region\": f.get(\"network_region\"),\n",
    "            \"lat\": (f.get(\"location\") or {}).get(\"lat\"),\n",
    "            \"lng\": (f.get(\"location\") or {}).get(\"lng\"),\n",
    "            \"created_at\": f.get(\"created_at\"),\n",
    "            \"updated_at\": f.get(\"updated_at\"),\n",
    "        })\n",
    "\n",
    "        for u in (f.get(\"units\") or []):\n",
    "                # unify field names we care about\n",
    "                unit_rows.append({\n",
    "                    \"unit_code\": u.get(\"code\"),\n",
    "                    \"facility_code\": f_code,\n",
    "                    \"fueltech_id\": u.get(\"fueltech_id\"),\n",
    "                    \"status_id\": u.get(\"status_id\"),\n",
    "                    \"dispatch_type\": u.get(\"dispatch_type\"),\n",
    "                    \"capacity_registered\": u.get(\"capacity_registered\"),\n",
    "                    \"capacity_maximum\": u.get(\"capacity_maximum\"),\n",
    "                    \"capacity_storage\": u.get(\"capacity_storage\"),\n",
    "                    \"data_first_seen\": u.get(\"data_first_seen\"),\n",
    "                    \"data_last_seen\": u.get(\"data_last_seen\"),\n",
    "                    \"unit_created_at\": u.get(\"created_at\"),\n",
    "                    \"unit_updated_at\": u.get(\"updated_at\"),\n",
    "                })\n",
    "    \n",
    "    facilities_df = pd.DataFrame(facility_rows).drop_duplicates(subset=[\"facility_code\"]).reset_index(drop=True)\n",
    "    units_lookup_df = pd.DataFrame(unit_rows).drop_duplicates(subset=[\"unit_code\"]).reset_index(drop=True)\n",
    "\n",
    "    return facilities_df, units_lookup_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the tables into separate csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\EXTRACTED\\electricity_facilities.csv\n",
      "Saved: DATA\\EXTRACTED\\electricity_units_facilities.csv\n"
     ]
    }
   ],
   "source": [
    "facilities_df, units_facilities_df = build_tables(facilities['data'])\n",
    "\n",
    "# save to csv\n",
    "save_dataset(facilities_df, \"DATA/EXTRACTED/electricity_facilities.csv\")\n",
    "save_dataset(units_facilities_df, \"DATA/EXTRACTED/electricity_units_facilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For power and emission data per facility, we need to perform some pre-processing to store them into a cached csv file. Specifically, for this process we need to:\n",
    "1. Combine all cached .json data of into one dataframe.\n",
    "2. Sum the facilitiy data to get total power and emissions per facility (some facilities have more than one units).\n",
    "3. Append additional information to each facility (e.g. lat, lon, facility_name, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all json cache into one dataframe\n",
    "records = []\n",
    "for path in glob.glob(\"./DATA/EXTRACTED/*.json\"):\n",
    "    payload = json.load(open(path))\n",
    "    \n",
    "    records.extend(flatten(payload))\n",
    "\n",
    "series_df = pd.DataFrame(records)\n",
    "\n",
    "lookup = pd.read_csv(\"./DATA/EXTRACTED/electricity_units_facilities.csv\")[[\"unit_code\",\"facility_code\"]]\n",
    "series_df = series_df.merge(lookup, on=\"unit_code\", how=\"left\")\n",
    "facility_df = (series_df.groupby([\"timestamp\",\"facility_code\",\"metric\"], as_index=False)[\"value\"].sum())\n",
    "\n",
    "facilities_df = pd.read_csv(\"./DATA/EXTRACTED/electricity_facilities.csv\")[\n",
    "    [\"facility_code\", \"facility_name\", \"network_id\", \"network_region\", \"lat\", \"lng\"]\n",
    "]\n",
    "facility_df = facility_df.merge(facilities_df, on=\"facility_code\", how=\"left\")\n",
    "\n",
    "# reorder columns for clarity\n",
    "facility_df = facility_df[\n",
    "    [\n",
    "        \"timestamp\", \"facility_code\", \"facility_name\", \"network_id\", \"network_region\",\n",
    "        \"lat\", \"lng\", \"metric\", \"value\"\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: DATA\\EXTRACTED\\consolidated_facilities.csv\n"
     ]
    }
   ],
   "source": [
    "# save to csv\n",
    "save_dataset(facility_df, \"DATA/EXTRACTED/consolidated_facilities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">3. Data Publishing via MTQQ</span></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MQTT Config\n",
    "BROKER = \"172.17.34.107\"   \n",
    "PORT = 1883\n",
    "TOPIC = \"COMP5339/facilities/rengga\"\n",
    "\n",
    "# define what happens upon connection to the server\n",
    "def on_connect(client, userdata, connect_flags, reason_code, properties):\n",
    "    print(\"Connected with result code \" + str(reason_code))\n",
    "    client.subscribe(\"COMP5339/ASSIGNMENT2/TUT07GR04\")\n",
    "\n",
    "# define what happens upon receiving a message from the server\n",
    "def on_message(client, userdata, msg):\n",
    "    print(f\"Received message on topic {msg.topic}: {msg.payload}\")\n",
    "\n",
    "# setup client\n",
    "client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)\n",
    "client.on_connect = on_connect\n",
    "client.connect(BROKER, PORT, 60)\n",
    "client.loop_start()\n",
    "\n",
    "df = pd.read_csv(\"DATA/EXTRACTED/consolidated_facilities.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> <span style=\"color:orange\">4. Dashboard and Visualisation</span></b>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_assignment2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
